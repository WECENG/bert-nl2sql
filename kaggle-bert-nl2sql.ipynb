{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 7237012,
     "sourceType": "datasetVersion",
     "datasetId": 4191013
    },
    {
     "sourceId": 7237059,
     "sourceType": "datasetVersion",
     "datasetId": 4191049
    },
    {
     "sourceId": 7252494,
     "sourceType": "datasetVersion",
     "datasetId": 4202144
    }
   ],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "#import\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "import torch.cuda\n",
    "import json\n",
    "from transformers import BertTokenizer,BertModel\n",
    "from torch import nn\n",
    "from sklearn import metrics\n",
    "from torch.optim.adam import Adam\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from typing import List"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-12-21T09:35:45.021093Z",
     "iopub.execute_input": "2023-12-21T09:35:45.021984Z",
     "iopub.status.idle": "2023-12-21T09:35:45.027189Z",
     "shell.execute_reply.started": "2023-12-21T09:35:45.021949Z",
     "shell.execute_reply": "2023-12-21T09:35:45.026176Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#utils\n",
    "def read_train_datas(path):\n",
    "    \"\"\"\n",
    "    :return: [[question, sel_col, conds:[col, op, start, end], conn_op],...]\n",
    "    \"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data_list = []\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            question = item['question']\n",
    "            sel = item['sql']['sel'][0]\n",
    "            cond_conn_op = item['sql']['cond_conn_op']\n",
    "            if item['sql'].get('conds') is not None:\n",
    "                conds = item['sql']['conds']\n",
    "                for i, cond in enumerate(conds):\n",
    "                    value = cond[2]\n",
    "                    start, end = value_start_end(question, value)\n",
    "                    cond[2] = start\n",
    "                    cond.append(end)\n",
    "            else:\n",
    "                conds = None\n",
    "            data_list.append([question, sel, conds, cond_conn_op])\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def value_start_end(question, value):\n",
    "    \"\"\"\n",
    "    get the start and end index of the value in the question\n",
    "    \"\"\"\n",
    "    question_length = len(question)\n",
    "    value_length = len(value)\n",
    "    for i in range(question_length - value_length + 1):\n",
    "        if question[i:value_length + i] == value:\n",
    "            return i, i + value_length - 1\n",
    "    return 0, 0\n",
    "\n",
    "\n",
    "def get_columns():\n",
    "    columns = ['基金代码', '基金名称', '成立时间', '基金类型', '基金规模', '销售状态', '是否可销售', '风险等级',\n",
    "               '基金公司名称', '分红方式',\n",
    "               '赎回状态', '是否支持定投', '净值同步日期', '净值', '成立以来涨跌幅', '昨日涨跌幅', '近一周涨跌幅',\n",
    "               '近一个月涨跌幅', '近三个月涨跌幅', '近六个月涨跌幅',\n",
    "               '近一年涨跌幅', '基金经理', '主题/概念', '一个月夏普率', '一年夏普率', '三个月夏普率', '六个月夏普率',\n",
    "               '成立以来夏普率', '投资市场', '板块', '行业',\n",
    "               '晨星三年评级', '管理费率', '销售服务费率', '托管费率', '认购费率', '申购费率', '赎回费率', '分红年度',\n",
    "               '权益登记日',\n",
    "               '除息日', '派息日', '红利再投日', '每十份收益单位派息', '主投资产类型', '基金投资风格描述', '估值',\n",
    "               '是否主动管理型基金', '投资', '跟踪指数',\n",
    "               '是否新发', '重仓', '无']\n",
    "    return columns\n",
    "\n",
    "\n",
    "def get_cond_op_dict():\n",
    "    cond_op_dict = {'>': 0, '<': 1, '==': 2, '!=': 3, 'like': 4, '>=': 5, '<=': 6, 'none': 7}\n",
    "    return cond_op_dict\n",
    "\n",
    "\n",
    "def get_conn_op_dict():\n",
    "    conn_op_dict = {'none': 0, 'and': 1, 'or': 2}\n",
    "    return conn_op_dict"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-21T09:35:47.462794Z",
     "iopub.execute_input": "2023-12-21T09:35:47.463660Z",
     "iopub.status.idle": "2023-12-21T09:35:47.476845Z",
     "shell.execute_reply.started": "2023-12-21T09:35:47.463628Z",
     "shell.execute_reply": "2023-12-21T09:35:47.475949Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#dataset\n",
    "# sql查询条件\n",
    "class Conditions(object):\n",
    "    def __init__(self, cond_col=None, cond_op=None, cond_value=None):\n",
    "        \"\"\"\n",
    "        example [cond_col cond_op cond_value]\n",
    "        :param cond_col: sql的查询条件列\n",
    "        :param cond_op: sql的查询条件操作符\n",
    "        :param cond_value: sql的查询条件值\n",
    "        \"\"\"\n",
    "        self.cond_col = cond_col\n",
    "        self.cond_op = cond_op\n",
    "        self.cond_value = cond_value\n",
    "\n",
    "\n",
    "# label\n",
    "class Label(object):\n",
    "    def __init__(self, label_sel_col=None, label_conn_op=None, label_cond: List[Conditions] = None):\n",
    "        \"\"\"\n",
    "        example [select label_sel_col from table where label_condition[0] label_conn_op label_condition[1] label_conn_op ...]\n",
    "        :param label_sel_col:\n",
    "        :param label_conn_op:\n",
    "        :param label_cond:\n",
    "        \"\"\"\n",
    "        self.label_sel_col = label_sel_col\n",
    "        self.label_conn_op = label_conn_op\n",
    "        self.label_cond = label_cond\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    def __init__(self, model_path=None, question_length=128, max_length=512, input_ids=None, attention_mask=None,\n",
    "                 token_type_ids=None, cls_idx=None, label=None):\n",
    "        if model_path is not None:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "        self.question_length = question_length\n",
    "        self.max_length = max_length\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.cls_idx = cls_idx\n",
    "        self.label = label\n",
    "\n",
    "    def encode_columns(self, columns: List):\n",
    "        \"\"\"\n",
    "        列编码\n",
    "        :param columns: 列\n",
    "        :return: 编码后的列，及序列号（用于列与列之间的区分）\n",
    "        \"\"\"\n",
    "        columns_encode = []\n",
    "        segment_ids = []\n",
    "        i = 1\n",
    "        for column in columns:\n",
    "            encod = self.tokenizer.encode(column)\n",
    "            seg = [i] * len(encod)\n",
    "            columns_encode.extend(encod)\n",
    "            segment_ids.extend(seg)\n",
    "            i = 1 - i  # 切换 0 和 1\n",
    "        return torch.tensor(columns_encode), torch.tensor(segment_ids)\n",
    "\n",
    "    def get_cls_idx(self, columns):\n",
    "        \"\"\"\n",
    "        获取列标记符的位置\n",
    "        :param columns: 列\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        cls_idx = []\n",
    "        start = self.question_length\n",
    "        for i in range(len(columns)):\n",
    "            cls_idx.append(int(start))\n",
    "            # 加上特殊标记的长度（例如 [CLS] 和 [SEP]）\n",
    "            start += len(columns[i]) + 2\n",
    "        return cls_idx\n",
    "\n",
    "    def encode_question_with_columns(self, que_length, max_length, question, columns_encode, columns_segment_id):\n",
    "        \"\"\"\n",
    "        编码\n",
    "        :param que_length: 问题长度\n",
    "        :param max_length: text长度\n",
    "        :param question:  问题\n",
    "        :param columns_encode:  编码的列\n",
    "        :param columns_segment_id 编码的列的序列\n",
    "        :return: 编码后的text\n",
    "        \"\"\"\n",
    "\n",
    "        # 编码问题，需要填充，否则会出现长度不一致异常\n",
    "        question_encoding = self.tokenizer.encode(question, add_special_tokens=True, padding='max_length',\n",
    "                                                  max_length=que_length, truncation=True)\n",
    "\n",
    "        # 合并编码后的张量，保证张量类型(dtype)为int或long, bert的embedding的要求\n",
    "        input_ids = torch.cat([torch.tensor(question_encoding), columns_encode], dim=0)\n",
    "        token_type_ids = torch.cat([torch.zeros(que_length, dtype=torch.long), columns_segment_id], dim=0)\n",
    "        padding_length = max_length - len(input_ids)\n",
    "        attention_mask = torch.cat([torch.ones(len(input_ids)), torch.zeros(padding_length)], dim=0)\n",
    "        input_ids = torch.cat([input_ids, torch.zeros(padding_length, dtype=torch.long)], dim=0)\n",
    "        token_type_ids = torch.cat([token_type_ids, torch.zeros(padding_length, dtype=torch.long)], dim=0)\n",
    "\n",
    "        return input_ids, attention_mask, token_type_ids\n",
    "\n",
    "    def list_features(self, datas):\n",
    "        \"\"\"\n",
    "        输入特征\n",
    "        :param datas: 数据\n",
    "        :param que_length: 问题长度\n",
    "        :param max_length: text长度\n",
    "        :return: 特征信息\n",
    "        \"\"\"\n",
    "        list_features = []\n",
    "        columns = get_columns()\n",
    "        cls_idx = self.get_cls_idx(columns)\n",
    "        columns_encode, columns_segment_id = self.encode_columns(get_columns())\n",
    "        for data in datas:\n",
    "            label = Label(label_sel_col=[data[1]], label_conn_op=[data[3]],\n",
    "                          label_cond=[Conditions(cond[0], cond[1], cond[2:4]) for cond in data[2]] if data[\n",
    "                                                                                                          2] is not None else None)\n",
    "            question = data[0]\n",
    "            # 编码(question+columns)\n",
    "            input_ids, attention_mask, token_type_ids = self.encode_question_with_columns(self.question_length,\n",
    "                                                                                          self.max_length,\n",
    "                                                                                          question, columns_encode,\n",
    "                                                                                          columns_segment_id)\n",
    "            list_features.append(\n",
    "                InputFeatures(question_length=self.question_length, max_length=self.max_length, input_ids=input_ids,\n",
    "                              attention_mask=attention_mask, token_type_ids=token_type_ids, cls_idx=cls_idx,\n",
    "                              label=label))\n",
    "        return list_features\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features: List[InputFeatures]):\n",
    "        self.features = features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        feature = self.features[item]\n",
    "        input_ids = np.array(feature.input_ids)\n",
    "        attention_mask = np.array(feature.attention_mask)\n",
    "        token_type_ids = np.array(feature.token_type_ids)\n",
    "        cls_idx = np.array(feature.cls_idx)\n",
    "        if feature.label is not None:\n",
    "            label: Label = feature.label\n",
    "            label_sel_col = np.array(label.label_sel_col)\n",
    "            label_conn_op = np.array(label.label_conn_op)\n",
    "            label_cond = np.array(label.label_cond)\n",
    "            if label_cond.any() is None or label_cond.size == 0:\n",
    "                # 初始化一维数组，保证纬度一致\n",
    "                # 52对应‘无’这一列\n",
    "                label_cond_col = np.array([len(get_columns()) - 1], dtype=np.int32)\n",
    "                # 7对应‘none’操作符\n",
    "                label_cond_op = np.array([len(get_cond_op_dict()) - 1], dtype=np.int32)\n",
    "                label_cond_value = np.array([[0, 0]], dtype=np.int32)\n",
    "            else:\n",
    "                # 转化成一维数组，保证纬度一致\n",
    "                label_cond_col = np.array([[item.cond_col] for item in label_cond]).ravel()[:np.prod(1)].reshape(\n",
    "                    1)\n",
    "                label_cond_op = np.array([[item.cond_op] for item in label_cond]).ravel()[:np.prod(1)].reshape(\n",
    "                    1)\n",
    "                label_cond_value = np.array([item.cond_value for item in label_cond]).ravel()[:np.prod((1, 2))].reshape(\n",
    "                    (1, 2))\n",
    "            # 打印样本信息\n",
    "#             print(f\"Sample {item}:\")\n",
    "#             print(f\"input_ids shape: {input_ids.shape}\")\n",
    "#             print(f\"attention_mask shape: {attention_mask.shape}\")\n",
    "#             print(f\"token_type_ids shape: {token_type_ids.shape}\")\n",
    "#             print(f\"label_sel_col shape: {label_sel_col.shape}\")\n",
    "#             print(f\"label_conn_op shape: {label_conn_op.shape}\")\n",
    "#             print(f\"label_cond_col shape: {label_cond_col.shape}\")\n",
    "#             print(f\"label_cond_op shape: {label_cond_op.shape}\")\n",
    "#             print(f\"label_cond_value shape: {label_cond_value.shape}\")\n",
    "            return input_ids, attention_mask, token_type_ids, cls_idx, label_sel_col, label_conn_op, label_cond_col, label_cond_op, label_cond_value\n",
    "        else:\n",
    "            return input_ids, attention_mask, token_type_ids, cls_idx\n",
    "\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-21T09:35:48.880138Z",
     "iopub.execute_input": "2023-12-21T09:35:48.880526Z",
     "iopub.status.idle": "2023-12-21T09:35:48.912034Z",
     "shell.execute_reply.started": "2023-12-21T09:35:48.880479Z",
     "shell.execute_reply": "2023-12-21T09:35:48.911149Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#model\n",
    "class ColClassifierModel(nn.Module):\n",
    "    def __init__(self, model_path, hidden_size, cond_op_length, dropout=0.5):\n",
    "        super(ColClassifierModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_path)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # todo 可以不止一列\n",
    "        self.sel_col_classifier = nn.Linear(hidden_size, 1)\n",
    "        # todo 条件不止一列\n",
    "        self.cond_col_classifier = nn.Linear(hidden_size, 1)\n",
    "        # out classes需要纬度必须大于label中size(classes)，否则会出现Assertion `t >= 0 && t < n_classes` failed.\n",
    "        self.cond_op_classifier = nn.Linear(hidden_size, cond_op_length)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, cls_idx=None):\n",
    "        # 输出最后一层隐藏状态以及池化层\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        dropout_output = self.dropout(outputs.pooler_output)\n",
    "        dropout_hidden_state = self.dropout(outputs.last_hidden_state)\n",
    "\n",
    "        \"\"\"\n",
    "        提取列特征信息，从dim=1即第二维中（列标记符号索引所在纬度）提取dropout_hidden_state对应该纬度的信息。\n",
    "        前提需要将cls_idx张量shape扩展成与dropout_hidden_state一致\n",
    "        \"\"\"\n",
    "        # cls_cols = dropout_hidden_state.gather(dim=1, index=cls_idx.unsqueeze(-1).expand(\n",
    "        #     dropout_hidden_state.shape[0], -1, dropout_hidden_state.shape[-1]))\n",
    "        # 简化写法\n",
    "        cls_cols = dropout_hidden_state[:, cls_idx[0], :]\n",
    "\n",
    "        out_sel_col = self.sel_col_classifier(cls_cols).squeeze(-1)\n",
    "        out_cond_col = self.cond_col_classifier(cls_cols).squeeze(-1)\n",
    "\n",
    "        out_cond_op = self.cond_op_classifier(dropout_output)\n",
    "\n",
    "        return out_sel_col, out_cond_col, out_cond_op\n",
    "\n",
    "\n",
    "class ValueClassifierModel(nn.Module):\n",
    "    def __init__(self, model_path, hidden_size, max_value_length, conn_op_length, question_length, dropout=0.5):\n",
    "        super(ValueClassifierModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_path)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # todo 最大条件值数量\n",
    "        self.cond_values_classifier = nn.Linear(hidden_size, max_value_length)\n",
    "        self.conn_op_classifier = nn.Linear(hidden_size, conn_op_length)\n",
    "        self.question_length = question_length\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n",
    "        # 输出最后一层隐藏状态以及池化层\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)\n",
    "        dropout_output = self.dropout(outputs.pooler_output)\n",
    "        dropout_hidden_state = self.dropout(outputs.last_hidden_state)\n",
    "\n",
    "        out_conn_op = self.conn_op_classifier(dropout_output)\n",
    "\n",
    "        # 提取问题特征信息\n",
    "        cond_values = dropout_hidden_state[:, 1:int(self.question_length), :]\n",
    "\n",
    "        out_cond_values = self.cond_values_classifier(cond_values)\n",
    "\n",
    "        return out_conn_op, out_cond_values"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-21T09:35:50.157871Z",
     "iopub.execute_input": "2023-12-21T09:35:50.158753Z",
     "iopub.status.idle": "2023-12-21T09:35:50.171757Z",
     "shell.execute_reply.started": "2023-12-21T09:35:50.158717Z",
     "shell.execute_reply": "2023-12-21T09:35:50.170765Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#train\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "def train(model: ColClassifierModel or ValueClassifierModel, model_save_path, train_dataset: Dataset,\n",
    "          val_dataset: Dataset, batch_size, lr, epochs):\n",
    "    # DataLoader根据batch_size获取数据，训练时选择打乱样本\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    # 是否使用gpu\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optim = Adam(model.parameters(), lr=lr)\n",
    "    if use_cuda:\n",
    "        model = model.to(device)\n",
    "        criterion = criterion.to(device)\n",
    "    best_val_avg_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        total_loss_train = 0\n",
    "        model.train()\n",
    "        # 训练进度\n",
    "        for input_ids, attention_mask, token_type_ids, cls_idx, label_sel_col, label_conn_op, label_cond_col, label_cond_op, label_cond_value in tqdm(\n",
    "                train_loader):\n",
    "            # model要求输入的矩阵(hidden_size,sequence_size),需要把第二纬度去除.squeeze(1)\n",
    "            input_ids = input_ids.squeeze(1).to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            token_type_ids = token_type_ids.squeeze(1).to(device)\n",
    "            if type(model) is ColClassifierModel:\n",
    "                label_sel_col = label_sel_col.squeeze(-1).to(device)\n",
    "                label_cond_col = label_cond_col.squeeze(-1).to(device)\n",
    "                label_cond_op = label_cond_op.squeeze(-1).to(device)\n",
    "                # 模型输出\n",
    "                out_sel_col, out_cond_col, out_cond_op = model(input_ids, attention_mask, token_type_ids, cls_idx)\n",
    "                # 计算损失\n",
    "                loss_sel_col = criterion(out_sel_col, label_sel_col)\n",
    "                loss_cond_col = criterion(out_cond_col, label_cond_col)\n",
    "                loss_cond_op = criterion(out_cond_op, label_cond_op)\n",
    "                # todo 损失比例\n",
    "                total_loss_train = loss_sel_col + loss_cond_col + loss_cond_op\n",
    "            if type(model) is ValueClassifierModel:\n",
    "                label_conn_op = label_conn_op.squeeze(-1).to(device)\n",
    "                label_cond_values = label_cond_value.squeeze(1).to(device)\n",
    "                # 模型输出\n",
    "                out_conn_op, out_cond_values = model(input_ids, attention_mask, token_type_ids)\n",
    "                # 计算损失\n",
    "                lost_conn_op = criterion(out_conn_op, label_conn_op)\n",
    "                lost_cond_values = criterion(out_cond_values, label_cond_values)\n",
    "                # todo 损失比例\n",
    "                total_loss_train = lost_conn_op + lost_cond_values\n",
    "            # 模型更新\n",
    "            model.zero_grad()\n",
    "            optim.zero_grad()\n",
    "            total_loss_train.backward()\n",
    "            optim.step()\n",
    "        # 模型验证\n",
    "        val_avg_acc = 0\n",
    "        out_all_sel_col = []\n",
    "        out_all_cond_col = []\n",
    "        out_all_cond_op = []\n",
    "        label_all_sel_col = []\n",
    "        label_all_cond_col = []\n",
    "        label_all_cond_op = []\n",
    "        out_all_conn_op = []\n",
    "        out_all_cond_values = []\n",
    "        label_all_conn_op = []\n",
    "        label_all_cond_values = []\n",
    "        # 验证无需梯度计算\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # 使用当前epoch训练好的模型验证\n",
    "            for input_ids, attention_mask, token_type_ids, cls_idx, label_sel_col, label_conn_op, label_cond_col, label_cond_op, label_cond_value in val_loader:\n",
    "                input_ids = input_ids.squeeze(1).to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                token_type_ids = token_type_ids.squeeze(1).to(device)\n",
    "                if type(model) is ColClassifierModel:\n",
    "                    label_sel_col = label_sel_col.squeeze(-1).to(device)\n",
    "                    label_cond_col = label_cond_col.squeeze(-1).to(device)\n",
    "                    label_cond_op = label_cond_op.squeeze(-1).to(device)\n",
    "                    # 模型输出\n",
    "                    out_sel_col, out_cond_col, out_cond_op = model(input_ids, attention_mask, token_type_ids, cls_idx)\n",
    "                    out_all_sel_col.append(out_sel_col.argmax(dim=1).cpu().numpy())\n",
    "                    out_all_cond_col.append(out_cond_col.argmax(dim=1).cpu().numpy())\n",
    "                    out_all_cond_op.append(out_cond_op.argmax(dim=1).cpu().numpy())\n",
    "                    label_all_sel_col.append(label_sel_col.cpu().numpy())\n",
    "                    label_all_cond_col.append(label_cond_col.cpu().numpy())\n",
    "                    label_all_cond_op.append(label_cond_op.cpu().numpy())\n",
    "                if type(model) is ValueClassifierModel:\n",
    "                    label_conn_op = label_conn_op.squeeze(-1).to(device)\n",
    "                    # reshape(-1)需要转成一维数组才能计算准确率\n",
    "                    label_cond_values = label_cond_value.squeeze(1).to(device).reshape(-1)\n",
    "                    # 模型输出\n",
    "                    out_conn_op, out_cond_values = model(input_ids, attention_mask, token_type_ids)\n",
    "                    out_all_conn_op.append(out_conn_op.argmax(dim=1).cpu().numpy())\n",
    "                    out_all_cond_values.append(out_cond_values.argmax(dim=1).reshape(-1).cpu().numpy())\n",
    "                    label_all_conn_op.append(label_conn_op.cpu().numpy())\n",
    "                    label_all_cond_values.append(label_cond_values.cpu().numpy())\n",
    "\n",
    "        if type(model) is ColClassifierModel:\n",
    "            val_sel_col_acc = metrics.accuracy_score(np.concatenate(out_all_sel_col), np.concatenate(label_all_sel_col))\n",
    "            val_cond_col_acc = metrics.accuracy_score(np.concatenate(out_all_cond_col),\n",
    "                                                      np.concatenate(label_all_cond_col))\n",
    "            val_cond_op_acc = metrics.accuracy_score(np.concatenate(out_all_cond_op), np.concatenate(label_all_cond_op))\n",
    "            # todo 准确率计算逻辑\n",
    "            val_avg_acc = (val_sel_col_acc + val_cond_col_acc + val_cond_op_acc) / 3\n",
    "        if type(model) is ValueClassifierModel:\n",
    "            val_conn_op_acc = metrics.accuracy_score(np.concatenate(out_all_conn_op), np.concatenate(label_all_conn_op))\n",
    "            val_cond_values_acc = metrics.accuracy_score(np.concatenate(out_all_cond_values),\n",
    "                                                         np.concatenate(label_all_cond_values))\n",
    "            val_avg_acc = (val_conn_op_acc + val_cond_values_acc) / 2\n",
    "        # save model\n",
    "        if val_avg_acc > best_val_avg_acc:\n",
    "            best_val_avg_acc = val_avg_acc\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f'''best model | Val Accuracy: {best_val_avg_acc: .3f}''')\n",
    "        print(\n",
    "            f'''Epochs: {epoch + 1} \n",
    "              | Train Loss: {total_loss_train: .3f} ]\n",
    "              | Val Accuracy: {val_avg_acc: .3f}''')\n",
    "\n",
    "\n",
    "def test(model, model_save_path, test_dataset, batch_size):\n",
    "    # 加载最佳模型权重\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.to(device)\n",
    "\n",
    "    total_acc_test = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_input, test_label in test_dataloader:\n",
    "            test_label = test_label.to(device)\n",
    "            attention_mask = test_input['attention_mask'].to(device)\n",
    "            input_ids = test_input['input_ids'].squeeze(1).to(device)\n",
    "            output = model(input_ids, attention_mask)\n",
    "            acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "            total_acc_test += acc\n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_dataset): .3f}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hidden_size = 768\n",
    "    batch_size = 64\n",
    "    learn_rate = 2e-5\n",
    "    epochs = 3\n",
    "    question_length = 128\n",
    "    max_length = 512\n",
    "    # 加载数据\n",
    "    label_datas = read_train_datas('/kaggle/input/bert-nl2sql-datas/waic_nl2sql_train.jsonl')\n",
    "    # 提取特征数据\n",
    "    list_input_features = InputFeatures('/kaggle/input/bert-nl2sql-chinese-model-hgd', question_length, max_length).list_features(label_datas)\n",
    "    # 初始化dataset\n",
    "    dateset = Dataset(list_input_features)\n",
    "    # 创建模型\n",
    "    colModel = ColClassifierModel('/kaggle/input/bert-nl2sql-chinese-model-hgd', hidden_size, len(get_cond_op_dict()))\n",
    "    valueModel = ValueClassifierModel('/kaggle/input/bert-nl2sql-chinese-model-hgd', hidden_size, 2, len(get_conn_op_dict()), question_length)\n",
    "    # 分割数据集\n",
    "    total_size = len(label_datas)\n",
    "    train_size = int(0.8 * total_size)\n",
    "    val_size = int(0.1 * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "    # 分割数据集\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dateset, [train_size, val_size, test_size])\n",
    "    print('train column model begin')\n",
    "    train(colModel, '/kaggle/working/classifier-column-model.pkl', train_dataset, val_dataset, batch_size, learn_rate, epochs)\n",
    "    print('train column model finish')\n",
    "    print('train value model begin')\n",
    "    train(valueModel, '/kaggle/working/classifier-value-model.pkl', train_dataset, val_dataset, batch_size, learn_rate,\n",
    "          epochs)\n",
    "    print('train value model finish')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-21T09:35:51.450296Z",
     "iopub.execute_input": "2023-12-21T09:35:51.450701Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "train value model begin\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "100%|██████████| 982/982 [13:56<00:00,  1.17it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "out_all_cond_values shape: (15704,)\nlabel_all_cond_values shape: (15704,)\nbest model | Val Accuracy:  0.994\nEpochs: 1 \n              | Train Loss:  0.053 ]\n              | Val Accuracy:  0.994\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": " 26%|██▋       | 259/982 [03:40<10:18,  1.17it/s]",
     "output_type": "stream"
    }
   ]
  }
 ]
}
