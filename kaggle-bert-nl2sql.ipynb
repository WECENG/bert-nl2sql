{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 7335893,
     "sourceType": "datasetVersion",
     "datasetId": 4258769
    },
    {
     "sourceId": 7335929,
     "sourceType": "datasetVersion",
     "datasetId": 4258788
    },
    {
     "sourceId": 7359402,
     "sourceType": "datasetVersion",
     "datasetId": 4274629
    },
    {
     "sourceId": 7359762,
     "sourceType": "datasetVersion",
     "datasetId": 4274874
    }
   ],
   "dockerImageVersionId": 30627,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "#import\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "import torch.cuda\n",
    "import json\n",
    "from transformers import BertTokenizer,BertModel\n",
    "from torch import nn\n",
    "from sklearn import metrics\n",
    "from torch.optim.adam import Adam\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from typing import List"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-01-08T04:36:09.948056Z",
     "iopub.execute_input": "2024-01-08T04:36:09.948497Z",
     "iopub.status.idle": "2024-01-08T04:36:13.030777Z",
     "shell.execute_reply.started": "2024-01-08T04:36:09.948453Z",
     "shell.execute_reply": "2024-01-08T04:36:13.029987Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#util\n",
    "def read_train_datas(path, question_length):\n",
    "    \"\"\"\n",
    "    :param path 数据路径\n",
    "    :param question_length 问题长度\n",
    "    :return: [[question, agg, conn_op, cond_ops, cond_vals],...], cond_vals:[[val_start_idx,val_end_idx],...]\n",
    "    \"\"\"\n",
    "    column_length = len(get_columns())\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data_list = []\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            # question\n",
    "            question = item['question']\n",
    "            # agg\n",
    "            sel = item['sql']['sel']\n",
    "            agg_op = item['sql']['agg']\n",
    "            agg = [get_agg_dict()['none']] * column_length\n",
    "            for i in range(len(sel)):\n",
    "                sel_col_item = sel[i]\n",
    "                agg_op_item = agg_op[i]\n",
    "                agg[sel_col_item] = agg_op_item\n",
    "            # conn_op\n",
    "            conn_op = item['sql']['cond_conn_op']\n",
    "            # cond_ops & cond_vals\n",
    "            cond_ops = [get_cond_op_dict()['none']] * column_length\n",
    "            cond_vals = [0] * question_length\n",
    "            if item['sql'].get('conds') is not None:\n",
    "                conds = item['sql']['conds']\n",
    "                for i, cond in enumerate(conds):\n",
    "                    cond_col_item = cond[0]\n",
    "                    cond_op_item = cond[1]\n",
    "                    cond_ops[cond_col_item] = cond_op_item\n",
    "                    value = cond[2]\n",
    "                    cond_vals = fill_value_start_end(cond_vals, question, value)\n",
    "            data_list.append([question, agg, conn_op, cond_ops, cond_vals])\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def read_predict_datas(path):\n",
    "    \"\"\"\n",
    "    :param path: 预测数据路径\n",
    "    :return: 预测数据\n",
    "    \"\"\"\n",
    "    questions = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            question = item['question']\n",
    "            questions.append([question])\n",
    "    return questions\n",
    "\n",
    "\n",
    "def fill_value_start_end(cond_vals, question, value):\n",
    "    \"\"\"\n",
    "    fill [1] by the value in the question\n",
    "    \"\"\"\n",
    "    question_length = len(question)\n",
    "    value_length = len(value)\n",
    "    for i in range(question_length - value_length + 1):\n",
    "        if question[i:value_length + i] == value:\n",
    "            cond_vals[i: value_length + i] = [1] * value_length\n",
    "    return cond_vals\n",
    "\n",
    "\n",
    "def get_columns():\n",
    "    columns = ['基金代码', '基金名称', '成立时间', '基金类型', '基金规模', '销售状态', '是否可销售', '风险等级',\n",
    "               '基金公司名称', '分红方式',\n",
    "               '赎回状态', '是否支持定投', '净值同步日期', '净值', '成立以来涨跌幅', '昨日涨跌幅', '近一周涨跌幅',\n",
    "               '近一个月涨跌幅', '近三个月涨跌幅', '近六个月涨跌幅',\n",
    "               '近一年涨跌幅', '基金经理', '主题/概念', '一个月夏普率', '一年夏普率', '三个月夏普率', '六个月夏普率',\n",
    "               '成立以来夏普率', '投资市场', '板块', '行业',\n",
    "               '晨星三年评级', '管理费率', '销售服务费率', '托管费率', '认购费率', '申购费率', '赎回费率', '分红年度',\n",
    "               '权益登记日',\n",
    "               '除息日', '派息日', '红利再投日', '每十份收益单位派息', '主投资产类型', '基金投资风格描述', '估值',\n",
    "               '是否主动管理型基金', '投资', '跟踪指数',\n",
    "               '是否新发', '重仓', '无']\n",
    "    return columns\n",
    "\n",
    "\n",
    "def get_cond_op_dict():\n",
    "    cond_op_dict = {'>': 0, '<': 1, '==': 2, '!=': 3, 'like': 4, '>=': 5, '<=': 6, 'none': 7}\n",
    "    return cond_op_dict\n",
    "\n",
    "\n",
    "def get_conn_op_dict():\n",
    "    conn_op_dict = {'none': 0, 'and': 1, 'or': 2}\n",
    "    return conn_op_dict\n",
    "\n",
    "\n",
    "def get_agg_dict():\n",
    "    agg_dict = {'': 0, 'AVG': 1, 'MAX': 2, 'MIN': 3, 'COUNT': 4, 'SUM': 5, 'none': 6}\n",
    "    return agg_dict\n",
    "\n",
    "\n",
    "def get_key(dict, value):\n",
    "    \"\"\"\n",
    "    根据字典的value获取key\n",
    "    :param dict: 字典\n",
    "    :param value: 值\n",
    "    :return: key\n",
    "    \"\"\"\n",
    "    return [k for k, v in dict.items() if v == value]\n",
    "\n",
    "\n",
    "def get_values_name(question, cond_vals):\n",
    "    \"\"\"\n",
    "    cond_vals的值如[0,1,1,1,1,0,0,0,1,1,1,0,0,0]所示\n",
    "    根据cond_vals中为1的值找到question对应下标的内容\n",
    "    返回找到的内容列表，连续为1的内容作为返回列表的一个元素\n",
    "    \"\"\"\n",
    "    question = question\n",
    "    result = []\n",
    "    cur_start_idx = 0\n",
    "    valid = False\n",
    "\n",
    "    for idx, val in enumerate(cond_vals):\n",
    "        if val == 1:\n",
    "            if not valid:\n",
    "                cur_start_idx = idx\n",
    "                valid = True\n",
    "        else:\n",
    "            if valid:\n",
    "                valid = False\n",
    "                if idx > cur_start_idx:\n",
    "                    vals = question[cur_start_idx:idx]\n",
    "                    result.append(vals)\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-08T04:36:15.932457Z",
     "iopub.execute_input": "2024-01-08T04:36:15.933138Z",
     "iopub.status.idle": "2024-01-08T04:36:15.961443Z",
     "shell.execute_reply.started": "2024-01-08T04:36:15.933099Z",
     "shell.execute_reply": "2024-01-08T04:36:15.960378Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#dataset\n",
    "# label\n",
    "class Label(object):\n",
    "    def __init__(self, label_agg: List = None, label_conn_op=None, label_cond_ops: List = None,\n",
    "                 label_cond_vals: List = None):\n",
    "        \"\"\"\n",
    "        训练标签信息\n",
    "        :param label_agg: 聚合函数\n",
    "        :param label_conn_op: 连接操作符\n",
    "        :param label_cond_ops: 条件操作符\n",
    "        :param label_cond_vals: 条件值\n",
    "        \"\"\"\n",
    "        self.label_agg = label_agg\n",
    "        self.label_conn_op = label_conn_op\n",
    "        self.label_cond_ops = label_cond_ops\n",
    "        self.label_cond_vals = label_cond_vals\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    def __init__(self, model_path=None, question_length=128, max_length=512, input_ids=None, attention_mask=None,\n",
    "                 token_type_ids=None, cls_idx=None, label: Label = None):\n",
    "        if model_path is not None:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "        self.question_length = question_length\n",
    "        self.max_length = max_length\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.cls_idx = cls_idx\n",
    "        self.label = label\n",
    "\n",
    "    def encode_expression(self, expressions: List):\n",
    "        \"\"\"\n",
    "        表达式编码\n",
    "        :param expressions: 表达式（列名或条件表达式）\n",
    "        :return: 编码后的列，及序列号（用于列与列之间的区分）\n",
    "        \"\"\"\n",
    "        encodings = self.tokenizer.batch_encode_plus(expressions)\n",
    "        expressions_encode = encodings[\"input_ids\"]\n",
    "        segment_ids = encodings[\"token_type_ids\"]\n",
    "        segment_ids = [[elem if j % 2 == 0 else 1 for elem in row] for j, row in enumerate(segment_ids)]\n",
    "        expressions_encode = [item for sublist in expressions_encode for item in sublist]\n",
    "        segment_ids = [item for sublist in segment_ids for item in sublist]\n",
    "        return torch.tensor(expressions_encode), torch.tensor(segment_ids)\n",
    "\n",
    "    def get_cls_idx(self, expressions):\n",
    "        \"\"\"\n",
    "        获取表达式标记符的位置\n",
    "        :param expressions: 表达式\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        cls_idx = []\n",
    "        start = self.question_length\n",
    "        for i in range(len(expressions)):\n",
    "            cls_idx.append(int(start))\n",
    "            # 加上特殊标记的长度（例如 [CLS] 和 [SEP]）\n",
    "            start += len(expressions[i]) + 2\n",
    "        return cls_idx\n",
    "\n",
    "    def encode_question_with_expressions(self, que_length, max_length, question, expressions_encode,\n",
    "                                         expressions_segment_id):\n",
    "        \"\"\"\n",
    "        编码\n",
    "        :param que_length: 问题长度\n",
    "        :param max_length: text长度\n",
    "        :param question:  问题\n",
    "        :param expressions_encode:  编码的列\n",
    "        :param expressions_segment_id 编码的列的序列\n",
    "        :return: 编码后的text\n",
    "        \"\"\"\n",
    "\n",
    "        # 编码问题，需要填充，否则会出现长度不一致异常\n",
    "        question_encoding = self.tokenizer.encode(question, add_special_tokens=True, padding='max_length',\n",
    "                                                  max_length=que_length, truncation=True)\n",
    "\n",
    "        # 合并编码后的张量，保证张量类型(dtype)为int或long, bert的embedding的要求\n",
    "        input_ids = torch.cat([torch.tensor(question_encoding), expressions_encode], dim=0)\n",
    "        token_type_ids = torch.cat([torch.zeros(que_length, dtype=torch.long), expressions_segment_id], dim=0)\n",
    "        padding_length = max_length - len(input_ids)\n",
    "        attention_mask = torch.cat([torch.ones(len(input_ids)), torch.zeros(padding_length)], dim=0)\n",
    "        input_ids = torch.cat([input_ids, torch.zeros(padding_length, dtype=torch.long)], dim=0)\n",
    "        token_type_ids = torch.cat([token_type_ids, torch.zeros(padding_length, dtype=torch.long)], dim=0)\n",
    "\n",
    "        return input_ids, attention_mask, token_type_ids\n",
    "\n",
    "    def list_features(self, datas):\n",
    "        \"\"\"\n",
    "        输入特征\n",
    "        :param datas: 数据\n",
    "        :return: 特征信息\n",
    "        \"\"\"\n",
    "        list_features = []\n",
    "        columns = get_columns()\n",
    "        cls_idx = self.get_cls_idx(columns)\n",
    "        expressions_encode, expressions_segment_id = self.encode_expression(columns)\n",
    "        for data in datas:\n",
    "            question = data[0]\n",
    "            # if contain label data\n",
    "            label = None\n",
    "            if len(data) > 1:\n",
    "                label = Label(label_agg=data[1], label_conn_op=data[2], label_cond_ops=data[3], label_cond_vals=data[4])\n",
    "            # 编码(question+expressions)\n",
    "            input_ids, attention_mask, token_type_ids = self.encode_question_with_expressions(self.question_length,\n",
    "                                                                                              self.max_length,\n",
    "                                                                                              question,\n",
    "                                                                                              expressions_encode,\n",
    "                                                                                              expressions_segment_id)\n",
    "            list_features.append(\n",
    "                InputFeatures(question_length=self.question_length, max_length=self.max_length, input_ids=input_ids,\n",
    "                              attention_mask=attention_mask, token_type_ids=token_type_ids, cls_idx=cls_idx,\n",
    "                              label=label))\n",
    "        return list_features\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features: List[InputFeatures]):\n",
    "        self.features = features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        feature = self.features[item]\n",
    "        input_ids = np.array(feature.input_ids)\n",
    "        attention_mask = np.array(feature.attention_mask)\n",
    "        token_type_ids = np.array(feature.token_type_ids)\n",
    "        cls_idx = np.array(feature.cls_idx)\n",
    "        if feature.label is not None:\n",
    "            label: Label = feature.label\n",
    "            label_agg = np.array(label.label_agg)\n",
    "            label_conn_op = np.array(label.label_conn_op)\n",
    "            label_cond_ops = np.array(label.label_cond_ops)\n",
    "            label_cond_vals = np.array([np.array(val) for val in label.label_cond_vals])\n",
    "            return input_ids, attention_mask, token_type_ids, cls_idx, label_agg, label_conn_op, label_cond_ops, label_cond_vals\n",
    "        else:\n",
    "            return input_ids, attention_mask, token_type_ids, cls_idx\n",
    "\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-08T04:36:17.092957Z",
     "iopub.execute_input": "2024-01-08T04:36:17.093327Z",
     "iopub.status.idle": "2024-01-08T04:36:17.118000Z",
     "shell.execute_reply.started": "2024-01-08T04:36:17.093297Z",
     "shell.execute_reply": "2024-01-08T04:36:17.117127Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#model\n",
    "class ColClassifierModel(nn.Module):\n",
    "    def __init__(self, model_path, hidden_size, agg_length, conn_op_length, cond_op_length, dropout=0.5):\n",
    "        super(ColClassifierModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_path)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # out classes需要纬度必须大于label中size(classes)，否则会出现Assertion `t >= 0 && t < n_classes` failed.\n",
    "        self.agg_classifier = nn.Linear(hidden_size, agg_length)\n",
    "        self.cond_ops_classifier = nn.Linear(hidden_size, cond_op_length)\n",
    "        self.conn_op_classifier = nn.Linear(hidden_size, conn_op_length)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, cls_idx=None):\n",
    "        # 输出最后一层隐藏状态以及池化层\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        dropout_output = self.dropout(outputs.pooler_output)\n",
    "        dropout_hidden_state = self.dropout(outputs.last_hidden_state)\n",
    "\n",
    "        \"\"\"\n",
    "        提取列特征信息，从dim=1即第二维中（列标记符号索引所在纬度）提取dropout_hidden_state对应该纬度的信息。\n",
    "        前提需要将cls_idx张量shape扩展成与dropout_hidden_state一致\n",
    "        \"\"\"\n",
    "        # cls_cols = dropout_hidden_state.gather(dim=1, index=cls_idx.unsqueeze(-1).expand(\n",
    "        #     dropout_hidden_state.shape[0], -1, dropout_hidden_state.shape[-1]))\n",
    "        # 简化写法\n",
    "        cls_cols = dropout_hidden_state[:, cls_idx[0], :]\n",
    "\n",
    "        out_agg = self.agg_classifier(cls_cols)\n",
    "        out_cond_ops = self.cond_ops_classifier(cls_cols)\n",
    "\n",
    "        out_conn_op = self.conn_op_classifier(dropout_output)\n",
    "\n",
    "        return out_agg, out_cond_ops, out_conn_op\n",
    "\n",
    "\n",
    "class ValueClassifierModel(nn.Module):\n",
    "    def __init__(self, model_path, hidden_size, question_length, cond_value_length=2, dropout=0.5):\n",
    "        super(ValueClassifierModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_path)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.cond_vals_classifier = nn.Linear(hidden_size, cond_value_length)\n",
    "        self.question_length = question_length\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n",
    "        # 输出最后一层隐藏状态\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = outputs.last_hidden_state\n",
    "\n",
    "        # 提取问题特征信息\n",
    "        cond_values = hidden_state[:, 1:self.question_length + 1, :]\n",
    "\n",
    "        out_cond_vals = self.cond_vals_classifier(cond_values)\n",
    "\n",
    "        return out_cond_vals\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-08T04:36:18.201383Z",
     "iopub.execute_input": "2024-01-08T04:36:18.202245Z",
     "iopub.status.idle": "2024-01-08T04:36:18.214251Z",
     "shell.execute_reply.started": "2024-01-08T04:36:18.202214Z",
     "shell.execute_reply": "2024-01-08T04:36:18.213337Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#train\n",
    "def train(model: ColClassifierModel or ValueClassifierModel, model_save_path, train_dataset: Dataset,\n",
    "          val_dataset: Dataset, batch_size, lr, epochs):\n",
    "    # DataLoader根据batch_size获取数据，训练时选择打乱样本\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    # 是否使用gpu\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optim = Adam(model.parameters(), lr=lr)\n",
    "    if use_cuda:\n",
    "        model = model.to(device)\n",
    "        criterion = criterion.to(device)\n",
    "    best_val_avg_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        total_loss_train = 0\n",
    "        model.train()\n",
    "        # 训练进度\n",
    "        for input_ids, attention_mask, token_type_ids, cls_idx, label_agg, label_conn_op, label_cond_ops, label_cond_vals in tqdm(\n",
    "                train_loader):\n",
    "            # model要求输入的矩阵(hidden_size,sequence_size),需要把第二纬度去除.squeeze(1)\n",
    "            input_ids = input_ids.squeeze(1).to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            token_type_ids = token_type_ids.squeeze(1).to(device)\n",
    "            if type(model) is ColClassifierModel:\n",
    "                # reshape(-1)合并一二纬度\n",
    "                label_agg = label_agg.to(device).reshape(-1)\n",
    "                label_conn_op = label_conn_op.to(device)\n",
    "                label_cond_ops = label_cond_ops.to(device).reshape(-1)\n",
    "                # 模型输出\n",
    "                out_agg, out_cond_ops, out_conn_op = model(input_ids, attention_mask, token_type_ids, cls_idx)\n",
    "                out_agg = out_agg.to(device).reshape(-1, out_agg.size(2))\n",
    "                out_cond_ops = out_cond_ops.to(device).reshape(-1, out_cond_ops.size(2))\n",
    "                out_conn_op = out_conn_op.to(device)\n",
    "                # 计算损失\n",
    "                loss_agg = criterion(out_agg, label_agg)\n",
    "                loss_conn_op = criterion(out_conn_op, label_conn_op)\n",
    "                loss_cond_ops = criterion(out_cond_ops, label_cond_ops)\n",
    "                # 损失比例\n",
    "                total_loss_train = loss_agg + loss_conn_op + loss_cond_ops\n",
    "\n",
    "            if type(model) is ValueClassifierModel:\n",
    "                label_cond_vals = label_cond_vals.to(device).reshape(-1)\n",
    "                # 模型输出\n",
    "                out_cond_vals = model(input_ids, attention_mask, token_type_ids)\n",
    "                # 计算损失\n",
    "                out_cond_vals = out_cond_vals.reshape(-1, out_cond_vals.size(2))\n",
    "                lost_cond_vals = criterion(out_cond_vals, label_cond_vals)\n",
    "                total_loss_train = lost_cond_vals\n",
    "\n",
    "            # 模型更新\n",
    "            model.zero_grad()\n",
    "            optim.zero_grad()\n",
    "            total_loss_train.backward()\n",
    "            optim.step()\n",
    "        # 模型验证\n",
    "        val_avg_acc = 0\n",
    "        out_all_agg = []\n",
    "        out_all_conn_op = []\n",
    "        out_all_cond_ops = []\n",
    "        out_all_cond_vals = []\n",
    "        label_all_agg = []\n",
    "        label_all_conn_op = []\n",
    "        label_all_cond_ops = []\n",
    "        label_all_cond_vals = []\n",
    "        # 验证无需梯度计算\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # 使用当前epoch训练好的模型验证\n",
    "            for input_ids, attention_mask, token_type_ids, cls_idx, label_agg, label_conn_op, label_cond_ops, label_cond_vals in val_loader:\n",
    "                input_ids = input_ids.squeeze(1).to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                token_type_ids = token_type_ids.squeeze(1).to(device)\n",
    "                if type(model) is ColClassifierModel:\n",
    "                    label_agg = label_agg.to(device).reshape(-1)\n",
    "                    label_conn_op = label_conn_op.to(device)\n",
    "                    label_cond_ops = label_cond_ops.to(device).reshape(-1)\n",
    "                    # 模型输出\n",
    "                    out_agg, out_cond_ops, out_conn_op = model(input_ids, attention_mask, token_type_ids, cls_idx)\n",
    "                    out_agg = out_agg.argmax(dim=2).to(device).reshape(-1)\n",
    "                    out_cond_ops = out_cond_ops.argmax(dim=2).to(device).reshape(-1)\n",
    "                    out_conn_op = out_conn_op.argmax(dim=1).to(device)\n",
    "                    out_all_agg.append(out_agg.cpu().numpy())\n",
    "                    out_all_conn_op.append(out_conn_op.cpu().numpy())\n",
    "                    out_all_cond_ops.append(out_cond_ops.cpu().numpy())\n",
    "                    label_all_agg.append(label_agg.cpu().numpy())\n",
    "                    label_all_conn_op.append(label_conn_op.cpu().numpy())\n",
    "                    label_all_cond_ops.append(label_cond_ops.cpu().numpy())\n",
    "                if type(model) is ValueClassifierModel:\n",
    "                    label_cond_vals = label_cond_vals.to(device).reshape(-1)\n",
    "                    # 模型输出\n",
    "                    out_cond_vals = model(input_ids, attention_mask, token_type_ids)\n",
    "                    out_cond_vals = out_cond_vals.argmax(dim=2).to(device).reshape(-1)\n",
    "                    out_all_cond_vals.append(out_cond_vals.cpu().numpy())\n",
    "                    label_all_cond_vals.append(label_cond_vals.cpu().numpy())\n",
    "\n",
    "        if type(model) is ColClassifierModel:\n",
    "            val_agg_acc = metrics.accuracy_score(np.concatenate(out_all_agg, axis=0),\n",
    "                                                 np.concatenate(label_all_agg, axis=0))\n",
    "            val_conn_op_acc = metrics.accuracy_score(np.concatenate(out_all_conn_op, axis=0),\n",
    "                                                     np.concatenate(label_all_conn_op, axis=0))\n",
    "            val_cond_ops_acc = metrics.accuracy_score(np.concatenate(out_all_cond_ops, axis=0),\n",
    "                                                      np.concatenate(label_all_cond_ops, axis=0))\n",
    "            # 准确率计算逻辑\n",
    "            val_avg_acc = (val_agg_acc + val_conn_op_acc + val_cond_ops_acc) / 3\n",
    "        if type(model) is ValueClassifierModel:\n",
    "            val_cond_vals_acc = metrics.accuracy_score(np.concatenate(out_all_cond_vals, axis=0),\n",
    "                                                       np.concatenate(label_all_cond_vals, axis=0))\n",
    "\n",
    "            val_avg_acc = val_cond_vals_acc\n",
    "        # save model\n",
    "        if val_avg_acc > best_val_avg_acc:\n",
    "            best_val_avg_acc = val_avg_acc\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f'''best model | Val Accuracy: {best_val_avg_acc: .3f}''')\n",
    "        print(\n",
    "            f'''Epochs: {epoch + 1} \n",
    "              | Train Loss: {total_loss_train.item(): .3f} \n",
    "              | Val Accuracy: {val_avg_acc: .3f}''')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hidden_size = 768\n",
    "    batch_size = 48\n",
    "    learn_rate = 2e-5\n",
    "    epochs = 3\n",
    "    question_length = 128\n",
    "    max_length = 512\n",
    "    train_data_path = '/kaggle/input/bert-nl2sql-datas/waic_nl2sql_train.jsonl'\n",
    "    pretrain_model_path = '/kaggle/input/bert-nl2sql-chinese-model-hgd'\n",
    "    save_column_model_path = '/kaggle/working/classifier-column-model.pkl'\n",
    "    save_value_model_path = '/kaggle/working/classifier-value-model.pkl'\n",
    "    # 加载数据\n",
    "    label_datas = label_datas = read_train_datas(train_data_path, question_length)\n",
    "    # 提取特征数据\n",
    "    col_model_features = InputFeatures(pretrain_model_path, question_length, max_length).list_features(label_datas)\n",
    "    # 初始化dataset\n",
    "    col_model_dateset = Dataset(col_model_features)\n",
    "    # 创建模型\n",
    "    col_model = ColClassifierModel(pretrain_model_path, hidden_size, len(get_agg_dict()), len(get_conn_op_dict()),\n",
    "                                   len(get_cond_op_dict()))\n",
    "    # 分割数据集\n",
    "    total_size = len(label_datas)\n",
    "    train_size = int(0.8 * total_size)\n",
    "    val_size = int(0.1 * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "    # 分割数据集\n",
    "    col_model_train_dataset, col_model_val_dataset, col_model_test_dataset = random_split(col_model_dateset,\n",
    "                                                                                          [train_size, val_size,\n",
    "                                                                                           test_size])\n",
    "    print('train column model begin')\n",
    "    train(col_model, save_column_model_path, col_model_train_dataset, col_model_val_dataset, batch_size, learn_rate,\n",
    "          epochs)\n",
    "    print('train column model finish')\n",
    "    value_model_features = InputFeatures(pretrain_model_path, question_length, max_length).list_features(label_datas)\n",
    "    value_model_dateset = Dataset(value_model_features)\n",
    "    value_model = ValueClassifierModel(pretrain_model_path, hidden_size, question_length)\n",
    "    value_model_train_dataset, value_model_val_dataset, value_model_test_dataset = random_split(value_model_dateset,\n",
    "                                                                                                [train_size, val_size,\n",
    "                                                                                                 test_size])\n",
    "    print('train value model begin')\n",
    "    train(value_model, save_value_model_path, value_model_train_dataset, value_model_val_dataset, batch_size,\n",
    "          learn_rate,\n",
    "          epochs)\n",
    "    print('train value model finish')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#predict\n",
    "def predict(questions, predict_result_path, pretrain_model_path, column_model_path, value_model_path, hidden_size,\n",
    "            batch_size, question_length, max_length, table_name='table_name'):\n",
    "    # 创建模型\n",
    "    col_model = ColClassifierModel(pretrain_model_path, hidden_size, len(get_agg_dict()), len(get_conn_op_dict()),\n",
    "                                   len(get_cond_op_dict()))\n",
    "    value_model = ValueClassifierModel(pretrain_model_path, hidden_size, question_length)\n",
    "    # 提取特征数据（不含label的数据）\n",
    "    input_features = InputFeatures(pretrain_model_path, question_length, max_length).list_features(questions)\n",
    "    dataset = Dataset(input_features)\n",
    "    # 预测不用打乱顺序shuffle=False\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    # 是否使用gpu\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        col_model = col_model.to(device)\n",
    "        value_model = value_model.to(device)\n",
    "        col_model.load_state_dict(torch.load(column_model_path, map_location=torch.device(device)))\n",
    "        value_model.load_state_dict(torch.load(value_model_path, map_location=torch.device(device)))\n",
    "    # 预测\n",
    "    pre_all_agg = []\n",
    "    pre_all_conn_op = []\n",
    "    pre_all_cond_ops = []\n",
    "    pre_all_cond_vals = []\n",
    "    for input_ids, attention_mask, token_type_ids, cls_idx in tqdm(dataloader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        out_agg, out_cond_ops, out_conn_op = col_model(input_ids, attention_mask, token_type_ids, cls_idx)\n",
    "\n",
    "        # 取预测结果最大值，torch.argmax找到指定纬度最大值所对应的索引（是索引，不是值）\n",
    "        pre_agg = torch.argmax(out_agg, dim=2).cpu().numpy()\n",
    "        pre_cond_ops = torch.argmax(out_cond_ops, dim=2).cpu().numpy()\n",
    "        pre_conn_op = torch.argmax(out_conn_op, dim=1).cpu().numpy()\n",
    "\n",
    "        pre_all_agg.extend(pre_agg)\n",
    "        pre_all_cond_ops.extend(pre_cond_ops)\n",
    "        pre_all_conn_op.extend(pre_conn_op)\n",
    "    for input_ids, attention_mask, token_type_ids, cls_idx in tqdm(dataloader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        out_cond_vals = value_model(input_ids, attention_mask, token_type_ids)\n",
    "        pre_cond_vals = torch.argmax(out_cond_vals, dim=2).cpu().numpy()\n",
    "        pre_all_cond_vals.extend(pre_cond_vals)\n",
    "\n",
    "    with open(predict_result_path, 'w', encoding='utf-8') as wf:\n",
    "        for question, agg, conn_op, cond_ops, cond_vals in zip(questions, pre_all_agg, pre_all_conn_op,\n",
    "                                                               pre_all_cond_ops, pre_all_cond_vals):\n",
    "            sel_col = np.where(np.array(agg) != get_agg_dict()['none'])[0]\n",
    "            agg = agg[agg != get_agg_dict()['none']]\n",
    "            cond_col = np.where(np.array(cond_ops) != get_cond_op_dict()['none'])[0]\n",
    "            cond_op = cond_ops[cond_ops != get_cond_op_dict()['none']]\n",
    "            sel_col_name = [get_columns()[idx_col] for idx_col in sel_col]\n",
    "            cond_vals_name = get_values_name(question[0], cond_vals)\n",
    "            conds = [[int(item_cond_col), int(item_cond_op), item_cond_val_name] for\n",
    "                     item_cond_col, item_cond_op, item_cond_val_name in zip(cond_col, cond_op, cond_vals_name)]\n",
    "            sql_dict = {\"question\": question, \"table_id\": table_name,\n",
    "                        \"sql\": {\"sel\": list(map(int, sel_col)),\n",
    "                                \"agg\": list(map(int, agg)),\n",
    "                                \"limit\": 0,\n",
    "                                \"orderby\": [],\n",
    "                                \"asc_desc\": 0,\n",
    "                                \"cond_conn_op\": int(conn_op),\n",
    "                                'conds': conds},\n",
    "                        \"keywords\": {\"sel_cols\": sel_col_name, \"values\": cond_vals_name}}\n",
    "            sql_json = json.dumps(sql_dict, ensure_ascii=False)\n",
    "            wf.write(sql_json + '\\n')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hidden_size = 768\n",
    "    batch_size = 24\n",
    "    question_length = 128\n",
    "    max_length = 512\n",
    "    predict_question_path = '/kaggle/input/bert-nl2sql-predict-datas/waic_nl2sql_testa_public.jsonl'\n",
    "    predict_result_path = '/kaggle/working/predict.jsonl'\n",
    "    pretrain_model_path = '/kaggle/input/bert-nl2sql-chinese-model-hgd'\n",
    "    column_model_path = '/kaggle/input/bert-nl2sql-result-model/classifier-column-model.pkl'\n",
    "    value_model_path = '/kaggle/input/bert-nl2sql-result-model/classifier-value-model.pkl'\n",
    "    questions = read_predict_datas(predict_question_path)\n",
    "    predict(questions, predict_result_path, pretrain_model_path, column_model_path, value_model_path, hidden_size,\n",
    "            batch_size, question_length, max_length)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-08T04:36:19.528975Z",
     "iopub.execute_input": "2024-01-08T04:36:19.529305Z",
     "iopub.status.idle": "2024-01-08T04:38:15.126041Z",
     "shell.execute_reply.started": "2024-01-08T04:36:19.529279Z",
     "shell.execute_reply": "2024-01-08T04:38:15.125032Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "text": "100%|██████████| 497/497 [00:54<00:00,  9.16it/s]\n100%|██████████| 497/497 [00:53<00:00,  9.26it/s]\n",
     "output_type": "stream"
    }
   ]
  }
 ]
}
